# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vrObitFD4nzYNSagt39ah_aBtQY2QY3W
"""

# 1 done
import numpy as np

def tsp_dist(x, mydist):
    """
    Calculate the total distance of a TSP tour.

    Parameters:
    x (list or array): A sequence of indices (floats or ints).
    mydist (2D array): A distance matrix where mydist[i][j] is the distance from i to j.

    Returns:
    float: The total distance of the tour.
    """
    x = np.round(x).astype(int)  # <- This ensures safe indexing
    x2 = np.column_stack((x[1:], x[:-1]))
    dist = sum(mydist[i, j] for i, j in x2)
    return dist

# 2 done
import numpy as np
from multiprocessing import Pool
import random

def boot_sdm(x, boot_R=999, ncpus=1, seed=None):
    """
    Bootstrap the sampling distribution of the mean.

    Parameters:
    x (array-like): Vector of observations.
    boot_R (int): Number of bootstrap resamples.
    ncpus (int): Number of cores to use.
    seed (int or None): Seed for the random number generator.

    Returns:
    np.ndarray: Vector of bootstrap estimates of the sample mean.
    """
    # Input checks
    if not isinstance(x, (list, np.ndarray)) or len(x) <= 1:
        raise ValueError("x must be a numeric list or array with more than one element.")
    if not isinstance(boot_R, int) or boot_R < 2:
        raise ValueError("boot_R must be an integer greater than 1.")
    if not isinstance(ncpus, int) or ncpus < 1:
        raise ValueError("ncpus must be a positive integer.")

    # Set random seed
    if seed is not None:
        np.random.seed(seed)
        random.seed(seed)

    x = np.array(x)

    # Define a function to generate a single bootstrap mean
    def bootstrap_mean(_):
        return np.mean(np.random.choice(x, size=len(x), replace=True))

    # Parallel or serial computation
    if ncpus > 1:
        with Pool(processes=ncpus) as pool:
            results = pool.map(bootstrap_mean, range(boot_R))
    else:
        results = [bootstrap_mean(i) for i in range(boot_R)]

    return np.array(results)

import matplotlib.pyplot as plt
import scipy.stats as stats

# Example 1: Normally distributed data
x = np.random.normal(loc=4, scale=1, size=15)
my_sdm = boot_sdm(x)
plt.hist(my_sdm, bins=30)
plt.title("Histogram of Bootstrap Means")
plt.show()

stats.probplot(my_sdm, dist="norm", plot=plt)
plt.title("QQ Plot of Bootstrap Means")
plt.show()

# 3 done
import numpy as np
from scipy.stats import t
from scipy.optimize import brentq

def power_t_test(delta, n, sig_level=0.05, alternative='two.sided'):
    """Calculate power of a paired t-test."""
    df = n - 1
    se = 1 / np.sqrt(n)
    t_val = delta / se
    if alternative == 'two.sided':
        crit_val = t.ppf(1 - sig_level / 2, df)
        beta = t.cdf(crit_val - t_val, df) - t.cdf(-crit_val - t_val, df)
        return 1 - beta
    elif alternative == 'one.sided':
        crit_val = t.ppf(1 - sig_level, df)
        beta = t.cdf(crit_val - t_val, df)
        return 1 - beta
    else:
        raise ValueError("Invalid alternative hypothesis type.")

def calc_instances(ncomparisons,
                   d=None,
                   ninstances=None,
                   power=None,
                   sig_level=0.05,
                   alternative='two.sided',
                   test='t.test',
                   power_target='mean'):
    """
    Calculate the required number of instances or the power for comparisons
    of multiple algorithms.
    """

    if sum(param is None for param in [d, ninstances, power]) != 1:
        raise ValueError("Exactly one of 'd', 'ninstances', or 'power' must be None.")

    test = test.lower()
    alternative = alternative.lower()
    power_target = power_target.lower()

    if test not in ['t.test', 'wilcoxon', 'binomial']:
        raise ValueError("Invalid test type.")
    if alternative not in ['one.sided', 'two.sided']:
        raise ValueError("Invalid alternative hypothesis.")
    if power_target not in ['mean', 'median', 'worst.case']:
        raise ValueError("Invalid power target.")

    # ARE correction factors
    correction_factor = {
        't.test': 1.0,
        'wilcoxon': 1 / 0.86,
        'binomial': 1 / 0.637
    }[test]

    if ninstances is None:
        # Estimate required number of instances
        def mean_power(n):
            powers = []
            for i in range(1, int(ncomparisons) + 1):
                alpha_i = sig_level / (int(ncomparisons) - i + 1)
                p = power_t_test(delta=d, n=int(n), sig_level=alpha_i, alternative=alternative)
                powers.append(p)
            return np.mean(powers)

        if power_target == 'mean':
            def objective(n):
                return mean_power(n) - power

            n_est = brentq(objective, 2, 1e3)
            final_n = int(np.ceil(n_est * correction_factor))

            powers = [power_t_test(d, int(n_est), sig_level=sig_level / (ncomparisons - i + 1),
                                   alternative=alternative) for i in range(1, int(ncomparisons) + 1)]

        else:
            if power_target == 'worst.case':
                r = 1
            elif power_target == 'median':
                r = ncomparisons // 2

            def target_power(n):
                alpha_r = sig_level / (ncomparisons - r + 1)
                return power_t_test(delta=d, n=int(n), sig_level=alpha_r, alternative=alternative) - power

            n_est = brentq(target_power, 2, 1e3)
            final_n = int(np.ceil(n_est * correction_factor))

            powers = [power_t_test(d, int(n_est), sig_level=sig_level / (ncomparisons - i + 1),
                                   alternative=alternative) for i in range(1, int(ncomparisons) + 1)]

        mean_power_val = np.mean(powers)
        median_power_val = np.median(powers)

    elif power is None:
        # Calculate power profile for given sample size
        corrected_n = int(ninstances / correction_factor)
        powers = [power_t_test(d, corrected_n, sig_level=sig_level / (ncomparisons - i + 1),
                               alternative=alternative) for i in range(1, int(ncomparisons) + 1)]
        final_n = ninstances
        mean_power_val = np.mean(powers)
        median_power_val = np.median(powers)

    return {
        'ninstances': final_n,
        'power': powers,
        'mean.power': mean_power_val,
        'median.power': median_power_val,
        'd': d,
        'sig.level': sig_level,
        'alternative': alternative,
        'test': test,
        'power.target': power_target
    }

# Example usage
K = 10
alpha = 0.05
target_power = 0.9
d = 0.5

# Case 1: Calculate required sample size
out = calc_instances(K, d, power=target_power, sig_level=alpha)

# Plot power of each comparison
plt.figure(figsize=(8, 5))
plt.plot(range(1, K + 1), out["power"], marker='o', linestyle='-', label='Individual powers')
plt.axhline(y=target_power, color='r', linestyle='--', label='Target power')
plt.title("Power of comparisons (mean design)")
plt.xlabel("Comparison")
plt.ylabel("Power")
plt.grid(True)
plt.text(1, 0.93, f"Mean power = {out['mean.power']:.2f} for N = {out['ninstances']}", fontsize=10)
plt.legend()
plt.tight_layout()
plt.show()

# 8 done
import numpy as np

def dummyinstance(distr, bias=0, **kwargs):
    """
    Generate one sample from the given distribution with optional bias.

    Parameters:
    - distr (str): Name of the distribution function ('rnorm', 'rexp', etc.)
    - bias (float): Value to add to the generated sample.
    - **kwargs: Parameters for the distribution (e.g., mean, sd, rate).

    Returns:
    - float: Sample value with bias.
    """
    # === Error checking ===
    if not isinstance(distr, str):
        raise TypeError("distr must be a string")
    if not isinstance(bias, (int, float)):
        raise TypeError("bias must be a numeric value")

    # Remove irrelevant keys
    kwargs.pop("FUN", None)
    kwargs.pop("alias", None)

    # One sample
    if distr == "rnorm":
        mean = kwargs.get("mean", 0)
        sd = kwargs.get("sd", 1)
        value = np.random.normal(loc=mean, scale=sd)
    elif distr == "rexp":
        rate = kwargs.get("rate", 1)
        value = np.random.exponential(scale=1/rate)
    elif distr == "runif":
        min_val = kwargs.get("min", 0)
        max_val = kwargs.get("max", 1)
        value = np.random.uniform(low=min_val, high=max_val)
    else:
        raise ValueError(f"Unsupported distribution: {distr}")

    return bias + value

# Example 1: Normal distribution
print(dummyinstance(distr="rnorm", mean=10, sd=1))

# Example 2: Centered exponential distribution (with bias)
lambda_ = 4
np.random.seed(1234)
y = [dummyinstance(distr="rexp", rate=lambda_, bias=-1/lambda_) for _ in range(10000)]

# Mean and histogram
print(np.mean(y))

# import matplotlib.pyplot as plt
plt.hist(y, bins=50)
plt.xlim(-0.5, 2.5)
plt.title("Centered Exponential Distribution")
plt.show()

# 7 done
import numpy as np


def dummyalgo(distribution_fun="rnorm",
              distribution_pars=None,
              instance=None):
    """
    Simulates a dummy algorithm using a specified distribution for offset
    and a dummy instance.
    """
    # Default parameters
    if distribution_pars is None:
        distribution_pars = {"mean": 0, "sd": 1}
    if instance is None:
        instance = {"FUN": "dummyinstance", "distr": "rnorm", "mean": 0, "sd": 1}

    # Error checking
    if not isinstance(distribution_fun, str):
        raise TypeError("distribution_fun must be a string")
    if not isinstance(distribution_pars, dict):
        raise TypeError("distribution_pars must be a dictionary")
    if not isinstance(instance, dict):
        raise TypeError("instance must be a dictionary")
    if not all(k in instance for k in ("FUN", "distr")):
        raise ValueError("instance must contain keys 'FUN' and 'distr'")

    # Generate offset
    if distribution_fun == "rnorm":
        mean = distribution_pars.get("mean", 0)
        sd = distribution_pars.get("sd", 1)
        offset = np.random.normal(loc=mean, scale=sd)
    elif distribution_fun == "runif":
        min_val = distribution_pars.get("min", 0)
        max_val = distribution_pars.get("max", 1)
        offset = np.random.uniform(low=min_val, high=max_val)
    else:
        raise ValueError(f"Unsupported distribution_fun: {distribution_fun}")

    # Generate instance value and return total
    y = offset + dummyinstance(**instance)
    return {"value": y}

instance = {"FUN": "dummyinstance", "distr": "rexp", "rate": 5, "bias": -1/5}

result = dummyalgo(
    distribution_fun="runif",
    distribution_pars={"min": -25, "max": 50},
    instance=instance
)

print(result)

# 9 little confusion
def gen_seq(x):
    """
    2-opt swap: Swap two random cities in the tour (excluding start/end if fixed).
    """
    x = x.copy()
    i, j = np.random.choice(len(x), size=2, replace=False)
    x[i], x[j] = x[j], x[i]
    return x

def example_SANN(temp, budget, instance):
    """
    Custom Simulated Annealing loop for TSP.

    Parameters:
    - temp (float): Initial temperature
    - budget (int): Number of iterations
    - instance (dict): Contains 'mydist' key with distance matrix

    Returns:
    dict: Best solution and its distance
    """
    dist_mat = np.array(instance['mydist'])
    n = dist_mat.shape[0]

    # Initial solution
    current = np.random.permutation(n)
    best = current.copy()
    current_cost = tsp_dist(current, dist_mat)
    best_cost = current_cost

    for step in range(budget):
        candidate = gen_seq(current)
        candidate_cost = tsp_dist(candidate, dist_mat)

        delta = candidate_cost - current_cost
        if delta < 0 or np.random.rand() < np.exp(-delta / temp):
            current = candidate
            current_cost = candidate_cost

            if current_cost < best_cost:
                best = current.copy()
                best_cost = current_cost

        # Cooling schedule (simple exponential)
        temp *= 0.995

    return {"solution": best, "cost": best_cost}

from scipy.spatial import distance_matrix
import pandas as pd

# Simulate eurodist using 10 European cities
cities = ["Paris", "London", "Berlin", "Madrid", "Rome", "Vienna", "Amsterdam", "Prague", "Lisbon", "Brussels"]
np.random.seed(42)
coords = np.random.rand(len(cities), 2) * 100  # fake lat/lon
eurodist = distance_matrix(coords, coords)

instance = {
    "FUN": "tsp_dist",
    "mydist": eurodist  # from earlier setup
}

res = example_SANN(temp=2000, budget=10000, instance=instance)

print("Best tour cost:", res["cost"])
print("Best tour path:", res["solution"])

# 10 done
def get_observations(algo, instance, n=1):
    """
    Generate `n` performance observations using a given algorithm and instance.

    Parameters:
    - algo (dict): Algorithm parameters. Must include a key "FUN" with a callable or function name.
    - instance (dict): Problem instance parameters.
    - n (int): Number of observations to generate.

    Returns:
    - list: A list of `n` observed performance values.
    """
    if not isinstance(n, int) or n < 0:
        raise ValueError("`n` must be a non-negative integer.")

    # Extract function
    fun = algo.get("FUN")
    if isinstance(fun, str):
        fun = globals()[fun]  # assumes the function is defined in global scope
    elif not callable(fun):
        raise ValueError("`FUN` must be a callable or the name of a function as a string.")

    # Remove 'FUN' and 'alias' from algo, add 'instance'
    myargs = {k: v for k, v in algo.items() if k not in ["FUN", "alias"]}
    myargs["instance"] = instance

    f = []
    for _ in range(n):
        result = fun(**myargs)
        f.append(result["value"])  # assumes result is a dict with key 'value'

    return f

# Define instance
instance = {
    "FUN": "dummyinstance",
    "distr": "rexp",
    "rate": 5,
    "bias": -1/5
}

algorithm = {
    "FUN": "dummyalgo",
    "distribution_fun": "runif",  # FIXED: was 'distribution.fun'
    "distribution_pars": {"min": -25, "max": 50}
}

# Register function names in global scope (if needed)
globals()["dummyinstance"] = dummyinstance
globals()["dummyalgo"] = dummyalgo

# Get 1000 observations
x = get_observations(algorithm, instance, n=1000)

# Plot (optional)
import matplotlib.pyplot as plt
plt.hist(x, bins=50)
plt.title("Histogram of observations")
plt.show()

# 14 done

import numpy as np
import pandas as pd
from itertools import combinations


def se_boot(Xk, dif="simple", comparisons="all.vs.all", boot_R=999):
    """
    Bootstrap estimation of performance difference (and its standard error) between algorithm observations.

    Parameters:
    - Xk: list of lists or arrays, each containing numeric observations from an algorithm.
    - dif: 'simple' or 'perc' for difference type.
    - comparisons: 'all.vs.all' or 'all.vs.first' for pairwise comparisons.
    - boot_R: Number of bootstrap replicates.

    Returns:
    - pandas.DataFrame: Summary of differences, standard errors, and optimal ratios.
    """

    # Error checking
    if not isinstance(Xk, list) or not all(isinstance(x, (list, np.ndarray)) for x in Xk):
        raise ValueError("Xk must be a list of numeric arrays/lists.")
    if not all(len(x) >= 2 for x in Xk):
        raise ValueError("Each element of Xk must have at least 2 values.")
    if dif not in ["simple", "perc"]:
        raise ValueError("`dif` must be 'simple' or 'perc'.")
    if comparisons not in ["all.vs.all", "all.vs.first"]:
        raise ValueError("`comparisons` must be 'all.vs.all' or 'all.vs.first'.")
    if not isinstance(boot_R, int) or boot_R <= 1:
        raise ValueError("`boot_R` must be an integer > 1.")

    nalgs = len(Xk)
    Nk = [len(x) for x in Xk]

    # Define pairs
    all_pairs = list(combinations(range(nalgs), 2))
    if comparisons == "all.vs.first":
        all_pairs = [(0, i) for i in range(1, nalgs)]

    Phik = []
    SEk = []
    Roptk = []

    for ind1, ind2 in all_pairs:
        phi_hat = np.zeros(boot_R)
        ropt_hat = np.zeros(boot_R)

        for i in range(boot_R):
            Xk_b = [np.random.choice(x, size=len(x), replace=True) for x in Xk]
            Vark = [np.var(x, ddof=1) for x in Xk_b]
            Xbark = [np.mean(x) for x in Xk_b]
            Xbar_all = np.mean(Xbark)

            if dif == "simple":
                phi_hat[i] = Xbark[ind1] - Xbark[ind2]
                ropt_hat[i] = np.sqrt(Vark[ind1] / Vark[ind2])
            elif dif == "perc":
                if comparisons == "all.vs.all":
                    phi_hat[i] = (Xbark[ind1] - Xbark[ind2]) / Xbar_all
                    ropt_hat[i] = np.sqrt(Vark[ind1] / Vark[ind2])
                elif comparisons == "all.vs.first":
                    phi_hat[i] = 1 - Xbark[ind2] / Xbark[ind1]
                    ropt_hat[i] = np.sqrt(Vark[ind1] / Vark[ind2]) * (Xbark[ind2] / Xbark[ind1])

        Phik.append(np.mean(phi_hat))
        SEk.append(np.std(phi_hat, ddof=1))
        Roptk.append(np.mean(ropt_hat))

    output = pd.DataFrame({
        "Alg1": [p[0] + 1 for p in all_pairs],
        "Alg2": [p[1] + 1 for p in all_pairs],
        "N1": [Nk[p[0]] for p in all_pairs],
        "N2": [Nk[p[1]] for p in all_pairs],
        "Phi": Phik,
        "SE": SEk,
        "r": [Nk[p[0]] / Nk[p[1]] for p in all_pairs],
        "ropt": Roptk
    })

    return output

import numpy as np

# Set seed for reproducibility
np.random.seed(1234)

# Create Xk: a list of 3 sets of normally distributed data
Xk = [
    np.random.normal(loc=5, scale=1, size=10),   # mean = 5, sd = 1
    np.random.normal(loc=10, scale=2, size=20),  # mean = 10, sd = 2
    np.random.normal(loc=15, scale=5, size=20)   # mean = 15, sd = 5
]

# Run se_boot in different configurations
res1 = se_boot(Xk, dif="simple", comparisons="all.vs.all")
res2 = se_boot(Xk, dif="perc", comparisons="all.vs.first")
res3 = se_boot(Xk, dif="perc", comparisons="all.vs.all")

# Display the results
print("=== Simple difference (all vs all) ===")
print(res1)

print("\n=== Percent difference (all vs first) ===")
print(res2)

print("\n=== Percent difference (all vs all) ===")
print(res3)

# 15 done

import numpy as np
import pandas as pd
from itertools import combinations

def se_param(Xk, dif="simple", comparisons="all.vs.all"):
    # Error checking
    if not isinstance(Xk, list) or not all(isinstance(x, (list, np.ndarray)) for x in Xk):
        raise ValueError("Xk must be a list of numeric arrays/lists.")
    if not all(len(x) >= 2 for x in Xk):
        raise ValueError("Each element of Xk must have at least 2 observations.")
    if dif not in ['simple', 'perc']:
        raise ValueError("Invalid 'dif' argument.")
    if comparisons not in ["all.vs.all", "all.vs.first"]:
        raise ValueError("Invalid 'comparisons' argument.")

    # Convert inputs to numpy arrays
    Xk = [np.array(x) for x in Xk]
    nalgs = len(Xk)
    Vark = np.array([np.var(x, ddof=1) for x in Xk])
    Xbark = np.array([np.mean(x) for x in Xk])
    Nk = np.array([len(x) for x in Xk])
    Xbar_all = np.mean(Xbark)

    # Determine algorithm pairs
    algo_pairs = list(combinations(range(nalgs), 2))
    if comparisons == "all.vs.first":
        algo_pairs = [(0, i) for i in range(1, nalgs)]

    Phik, SEk, Roptk = [], [], []

    for ind1, ind2 in algo_pairs:
        if dif == "simple":
            phi = Xbark[ind1] - Xbark[ind2]
            se = np.sqrt(Vark[ind1] / Nk[ind1] + Vark[ind2] / Nk[ind2])
            ropt = np.sqrt(Vark[ind1] / Vark[ind2])
        elif dif == "perc":
            if comparisons == "all.vs.all":
                phi = (Xbark[ind1] - Xbark[ind2]) / Xbar_all
                C1 = (1 + (phi ** 2) / (nalgs ** 2)) / (Xbar_all ** 2)
                C2 = np.sum(Vark[[i for i in range(nalgs) if i not in [ind1, ind2]]] /
                            Nk[[i for i in range(nalgs) if i not in [ind1, ind2]]]) * \
                     (phi ** 2) / (nalgs ** 2 * Xbar_all ** 2)
                se = np.sqrt(C1 * (Vark[ind1] / Nk[ind1] + Vark[ind2] / Nk[ind2]) + C2)
                ropt = np.sqrt(Vark[ind1] / Vark[ind2])
            elif comparisons == "all.vs.first":
                phi = 1 - Xbark[ind2] / Xbark[ind1]
                C1 = Vark[ind1] * (Xbark[ind2] / (Xbark[ind1] ** 2)) ** 2
                C2 = Vark[ind2] / (Xbark[ind1] ** 2)
                se = np.sqrt(C1 / Nk[ind1] + C2 / Nk[ind2])
                ropt = np.sqrt(Vark[ind1] / Vark[ind2]) * (Xbark[ind2] / Xbark[ind1])
        else:
            raise ValueError("Unrecognized 'dif' option")

        Phik.append(phi)
        SEk.append(se)
        Roptk.append(ropt)

    result = pd.DataFrame({
        "Alg1": [p[0] + 1 for p in algo_pairs],
        "Alg2": [p[1] + 1 for p in algo_pairs],
        "N1": [Nk[p[0]] for p in algo_pairs],
        "N2": [Nk[p[1]] for p in algo_pairs],
        "Phi": Phik,
        "SE": SEk,
        "r": [Nk[p[0]] / Nk[p[1]] for p in algo_pairs],
        "ropt": Roptk
    })

    return result

import numpy as np

np.random.seed(1234)
Xk = [np.random.normal(5, 1, 10),
      np.random.normal(10, 2, 20),
      np.random.normal(15, 5, 20)]

# Example usage
print(se_param(Xk, dif="simple", comparisons="all.vs.all"))
print(se_param(Xk, dif="perc", comparisons="all.vs.first"))
print(se_param(Xk, dif="perc", comparisons="all.vs.all"))

# 5 done

import numpy as np
import pandas as pd

def calc_se(Xk, dif="simple", comparisons="all.vs.all", method="param", boot_R=999):
    """
    Calculate standard errors using parametric or bootstrap methods.

    Parameters:
        Xk (list of arrays): List of performance observations for each algorithm.
        dif (str): Type of difference ('simple' or 'perc').
        comparisons (str): Type of comparisons ('all.vs.all' or 'all.vs.first').
        method (str): Method of standard error estimation ('param' or 'boot').
        boot_R (int): Number of bootstrap resamples.

    Returns:
        pd.DataFrame: Resulting standard errors and related statistics.
    """

    # === Error checking ===
    if not isinstance(Xk, list) or not all(isinstance(x, (list, np.ndarray)) for x in Xk):
        raise ValueError("Xk must be a list of numeric arrays or lists.")
    if not all(len(x) >= 2 for x in Xk):
        raise ValueError("Each item in Xk must have at least two observations.")
    if dif not in ['simple', 'perc']:
        raise ValueError("dif must be 'simple' or 'perc'.")
    if comparisons not in ['all.vs.all', 'all.vs.first']:
        raise ValueError("comparisons must be 'all.vs.all' or 'all.vs.first'.")
    if method not in ['param', 'boot']:
        raise ValueError("method must be 'param' or 'boot'.")
    if not isinstance(boot_R, int) or boot_R < 1:
        raise ValueError("boot_R must be an integer greater than 0.")

    # === Calculation ===
    if method == "param":
        Diffk = se_param(Xk, dif=dif, comparisons=comparisons)
    else:
        Diffk = se_boot(Xk, dif=dif, comparisons=comparisons, boot_R=boot_R)

    # === Fix NaN values in SE column ===
    Diffk['SE'] = np.nan_to_num(Diffk['SE'], nan=0.0)

    return Diffk

import numpy as np

# Set seed for reproducibility
np.random.seed(1234)

# Create the list of observations Xk
Xk = [
    np.random.normal(5, 1, 10),    # mean = 5, sd = 1, n = 10
    np.random.normal(10, 2, 20),   # mean = 10, sd = 2, n = 20
    np.random.normal(15, 5, 50)    # mean = 15, sd = 5, n = 50
]

# Run different configurations of calc_se
print("== simple | all.vs.all | param ==")
print(calc_se(Xk, dif="simple", comparisons="all.vs.all", method="param"))

print("\n== simple | all.vs.all | boot ==")
print(calc_se(Xk, dif="simple", comparisons="all.vs.all", method="boot"))

print("\n== perc | all.vs.first | param ==")
print(calc_se(Xk, dif="perc", comparisons="all.vs.first", method="param"))

print("\n== perc | all.vs.first | boot ==")
print(calc_se(Xk, dif="perc", comparisons="all.vs.first", method="boot"))

print("\n== perc | all.vs.all | param ==")
print(calc_se(Xk, dif="perc", comparisons="all.vs.all", method="param"))

print("\n== perc | all.vs.all | boot ==")
print(calc_se(Xk, dif="perc", comparisons="all.vs.all", method="boot"))

# 4 done

import os
import time
import random
import pickle
import numpy as np
from multiprocessing import Pool
from typing import List, Dict, Any

def get_obs_wrapper(args):
    algo, n, instance = args
    return get_observations(algo=algo, n=n, instance=instance)


def get_worst_index(Diffk):
    idx = np.argmax(Diffk["SE"].values)  # np.argmax on numpy array
    return Diffk.iloc[idx]


def calc_nreps(instance: Dict[str, Any],
               algorithms: List[Dict[str, Any]],
               se_max: float,
               dif: str = "simple",
               comparisons: str = "all.vs.all",
               method: str = "param",
               nstart: int = 20,
               nmax: int = 1000,
               seed: int = None,
               boot_R: int = 499,
               ncpus: int = 1,
               force_balanced: bool = False,
               load_folder: str = None,
               save_folder: str = None):
    # Basic assertions
    assert isinstance(instance, dict) and "FUN" in instance
    assert isinstance(algorithms, list) and all(isinstance(a, dict) and "FUN" in a for a in algorithms)
    assert isinstance(se_max, (float, int)) and dif in ["simple", "perc"]
    assert comparisons in ["all.vs.all", "all.vs.first"]
    assert method in ["param", "boot"]
    assert isinstance(nstart, int) and isinstance(nmax, int)
    assert nmax >= len(algorithms) * nstart
    if seed is not None:
        assert isinstance(seed, int)

    if seed is None:
        seed = int(time.time())
    random.seed(seed)
    np.random.seed(seed)

    if "alias" not in instance:
        instance["alias"] = instance["FUN"]

    for algo in algorithms:
        if "alias" not in algo:
            algo["alias"] = algo["FUN"]

    Xk = {algo["alias"]: [] for algo in algorithms}
    Nk = {alias: 0 for alias in Xk.keys()}

    # Load results if needed
    if load_folder:
        filepath = os.path.join(load_folder, f"{instance['alias']}.pkl")
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                data_in_file = pickle.load(f)
            for alias in data_in_file["Nk"]:
                if alias in Xk:
                    Xk[alias] = data_in_file["Xk"][alias]
                    Nk[alias] = data_in_file["Nk"][alias]
                    print(f"{Nk[alias]} observations retrieved for algorithm: {alias}")
        else:
            print(f"NOTE: Instance file '{filepath}' not found.")
    n_loaded = Nk.copy()

    print(f"Sampling algorithms on instance {instance['alias']}:")

    n0 = {}
    for alias in Nk:
        if force_balanced:
            required = max(Nk[alias], nstart)
        else:
            required = max(0, nstart - Nk[alias])
        n0[alias] = max(0, required)

    # Parallel sampling for initial nstart samples
    with Pool(ncpus) as p:
        newX_list = p.map(get_obs_wrapper, [(algo, n0[algo["alias"]], instance) for algo in algorithms])

    for algo, newX in zip(algorithms, newX_list):
        alias = algo["alias"]
        Xk[alias].extend(newX)
        Nk[alias] = len(Xk[alias])

    Xk_list = [Xk[alias] for alias in Xk]

    Diffk = calc_se(Xk=Xk_list, dif=dif, comparisons=comparisons, method=method, boot_R=boot_R)

    aliases = [algo["alias"] for algo in algorithms]

    while (Diffk["SE"] > se_max).any() and (sum(Nk.values()) - sum(n_loaded.values()) < nmax):
        if sum(Nk.values()) % nstart == 0:
            print(".", end="", flush=True)

        n = {alias: 0 for alias in Xk.keys()}
        if force_balanced:
            for alias in n:
                n[alias] = 1
        else:
            worst = get_worst_index(Diffk)
            a1, a2 = worst["Alg1"], worst["Alg2"]
            alias1 = aliases[int(a1) - 1]
            alias2 = aliases[int(a2) - 1]

            chosen = alias1 if worst["r"] <= worst["ropt"] else alias2
            n[chosen] = 1

        with Pool(ncpus) as p:
            newX_list = p.map(get_obs_wrapper, [(algo, n[algo["alias"]], instance) for algo in algorithms])

        for algo, newX in zip(algorithms, newX_list):
            alias = algo["alias"]
            if n[alias] > 0:
                Xk[alias].extend(newX)
                Nk[alias] = len(Xk[alias])

        Xk_list = [Xk[alias] for alias in Xk]
        Diffk = calc_se(Xk=Xk_list, dif=dif, comparisons=comparisons, method=method, boot_R=boot_R)

    output = {
        "instance": instance["alias"],
        "Xk": Xk,
        "Nk": Nk,
        "n_loaded": n_loaded,
        "Diffk": Diffk,
        "dif": dif,
        "method": method,
        "comparisons": comparisons,
        "seed": seed
    }

    if save_folder:
        os.makedirs(save_folder, exist_ok=True)
        save_path = os.path.join(save_folder, f"{instance['alias']}.pkl")
        with open(save_path, 'wb') as f:
            pickle.dump(output, f)
        print(f"\nWriting file {os.path.basename(save_path)}")

    return output

import numpy as np
import pandas as pd
from typing import Callable, List, Dict

means = [15, 10, 30, 15, 20]
sds = [2, 4, 6, 8, 10]

algorithms = [
    {
        "FUN": "dummyalgo",  # or dummyalgo if you're using a function, see note below
        "alias": f"algo{i+1}",
        "distribution_fun": "rnorm",  #
        "distribution_pars": {"mean": m, "sd": s}
    }
    for i, (m, s) in enumerate(zip(means, sds))
]


instance = {
    "FUN": "dummyinstance",  # or dummyinstance if using function pointer
    "distr": "rexp",         #
    "rate": 5,
    "bias": -1/5
}


# Parameters for calc_nreps
params = {
    "se_max": 0.05,
    "dif": "perc",
    "comparisons": "all.vs.all",
    "method": "param",
    "nstart": 15,
    "nmax": 1000,
    "seed": 1234,
    "boot_R": 499,
    "ncpus": 1,
    "force_balanced": False,
    "load_folder": None,
    "save_folder": None
}

# Placeholder for `calc_nreps` Python implementation (not yet defined)
# We will define this function to simulate adaptive sampling later

algorithms, instance, params

myreps = calc_nreps(instance   = instance,
                        algorithms = algorithms,
                        se_max     = 0.05,          # desired (max) standard error
                        dif        = "perc",        # type of difference
                        comparisons = "all.vs.all", # differences to consider
                        method     = "param",       # method ("param", "boot")
                        nstart     = 15,            # initial number of samples
                        nmax       = 1000,          # maximum allowed sample size
                        seed       = 1234,          # seed for PRNG
                        boot_R     = 499,           # number of bootstrap resamples (unused)
                        ncpus      = 1,             # number of cores to use
                        force_balanced = False,     # force balanced sampling?
                        load_folder   = None,         # file to load results from
                        save_folder = None)         # folder to save results

# 13 done

import os
import time
import random
import pickle
import numpy as np
import pandas as pd
from typing import List, Dict, Any
from multiprocessing import Pool, cpu_count


def run_experiment(instances: List[Dict[str, Any]],
                    algorithms: List[Dict[str, Any]],
                    d: float,
                    se_max: float,
                    power: float = 0.8,
                    sig_level: float = 0.05,
                    power_target: str = "mean",
                    dif: str = "simple",
                    comparisons: str = "all.vs.all",
                    alternative: str = "two.sided",
                    test: str = "t.test",
                    method: str = "param",
                    nstart: int = 20,
                    nmax: int = None,
                    force_balanced: bool = False,
                    ncpus: int = 2,
                    boot_R: int = 499,
                    seed: int = None,
                    save_partial_results: str = None,
                    load_partial_results: str = None,
                    save_final_result: str = None):

    if alternative in ["less", "greater"]:
        assert comparisons == "all.vs.first"
        alternative_side = "one.sided"
    else:
        alternative_side = "two.sided"

    if dif.lower() == "percent":
        dif = "perc"

    if seed is None:
        seed = int(time.time())
    random.seed(seed)
    np.random.seed(seed)

    var_input_pars = locals()

    if os.name == 'nt' and ncpus > 1:
        print("\nMulticore not available on Windows. Forcing ncpus = 1.")
        ncpus = 1
    else:
        available_cores = cpu_count()
        if ncpus >= available_cores:
            print(f"\nncpus too large, using {available_cores - 1} cores.")
            ncpus = available_cores - 1

    for inst in instances:
        inst.setdefault("alias", inst["FUN"])

    for algo in algorithms:
        algo.setdefault("alias", algo["FUN"])

    n_available = len(instances)
    n_algs = len(algorithms)
    n_comparisons = n_algs * (n_algs - 1) / 2 if comparisons == "all.vs.all" else n_algs - 1

    if power >= 1:
        ss_calc = calc_instances(ncomparisons=n_comparisons, d=d,
                                 ninstances=n_available, sig_level=sig_level,
                                 alternative=alternative_side, test=test,
                                 power_target=power_target)
        N_star = n_available
    else:
        ss_calc = calc_instances(ncomparisons=n_comparisons, d=d,
                                 power=power, sig_level=sig_level,
                                 alternative=alternative_side, test=test,
                                 power_target=power_target)
        N_star = int(np.ceil(ss_calc['ninstances']))
        if N_star < n_available:
            random.shuffle(instances)

    inst_to_use = min(N_star, n_available)

    print("CAISEr running")
    print("-----------------------------")
    print(f"Required number of instances: {N_star}")
    print(f"Available number of instances: {n_available}")
    print(f"Using {ncpus} cores.")
    print("-----------------------------")

    selected_instances = instances[:inst_to_use]

    if ncpus > 1:
        with Pool(ncpus) as pool:
            my_results = pool.starmap(calc_nreps, [(inst, algorithms, se_max, dif, comparisons,
                                                    method, nstart, nmax or 100 * len(algorithms),
                                                    seed, boot_R, 1, force_balanced,
                                                    load_partial_results, save_partial_results)
                                                   for inst in selected_instances])
    else:
        my_results = [calc_nreps(inst, algorithms, se_max, dif, comparisons,
                                 method, nstart, nmax or 100 * len(algorithms),
                                 seed, boot_R, 1, force_balanced,
                                 load_partial_results, save_partial_results)
                      for inst in selected_instances]

    data_raw = []
    for res in my_results:
        inst = res['instance']
        total_n = sum(res['Nk'].values())
        for algo_name, obs_list in res['Xk'].items():
            for val in obs_list:
                data_raw.append({"Algorithm": algo_name,
                                 "Instance": inst,
                                 "Observation": val})

    data_raw_df = pd.DataFrame(data_raw)

    data_summary = []
    for res in my_results:
        df = res['Diffk'].copy()
        df['Instance'] = res['instance']
        data_summary.append(df)

    data_summary_df = pd.concat(data_summary, ignore_index=True)
    algonames = [algo['alias'] for algo in algorithms]
    data_summary_df['Alg1'] = data_summary_df['Alg1'].apply(lambda i: algonames[int(i) - 1])
    data_summary_df['Alg2'] = data_summary_df['Alg2'].apply(lambda i: algonames[int(i) - 1])

    output = {
        "Configuration": var_input_pars,
        "data.raw": data_raw_df,
        "data.summary": data_summary_df,
        "N": inst_to_use,
        "N.star": N_star,
        "total.runs": len(data_raw_df),
        "instances.sampled": data_raw_df['Instance'].unique().tolist(),
        "Underpowered": N_star > n_available,
        "samplesize.calc": ss_calc
    }

    if save_final_result:
        os.makedirs(save_final_result, exist_ok=True)
        filename = time.strftime("CAISEr_results_%Y%m%d%H%M%S.pkl")
        filepath = os.path.join(save_final_result, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(output, f)
        print(f"\nWriting file {filename}")

    return output

import numpy as np
import random

# Create list of algorithms with distribution parameters
algorithms = [
    {
        "FUN": "dummyalgo",
        "alias": f"algo{i}",
        "distribution_fun": "rnorm",
        "distribution_pars": {"mean": m, "sd": s}
    }
    for i, m, s in zip([1, 2, 3, 4], [15, 10, 30, 15], [2, 4, 6, 8])
]

# Generate 100 dummy instances with exponential distribution
instances = []
for i in range(1, 101):
    rate = random.uniform(1, 10)
    instances.append({
        "FUN": "dummyinstance",
        "alias": f"Inst.{i}",
        "distr": "rexp",
        "rate": rate,
        "bias": -1 / rate
    })

# Run experiment (assuming run_experiment is defined elsewhere)
my_results = run_experiment(
    instances, algorithms,
    d=0.5, se_max=0.1,
    power=0.9, sig_level=0.05,
    power_target="mean",
    dif="perc", comparisons="all.vs.all",
    ncpus=1, seed=1234
)

# 16 done

def summary_CAISEr(object, test=None, alternative=None, sig_level=None):
    import numpy as np
    from scipy import stats

    # Set default parameters from object
    if test is None:
        test = object['Configuration']['test']
    if alternative is None:
        alternative = object['Configuration']['alternative']
    if sig_level is None:
        sig_level = object['Configuration']['sig_level']

    # Input validation
    assert test in ['t.test', 'wilcoxon', 'binomial'], f"Unknown test: {test}"
    assert alternative in ['less', 'greater', 'two-sided']
    assert 0 < sig_level < 1

    # Extract data
    data_raw = object['data.raw']
    data_summary = object['data.summary']
    alpha_list = object['samplesize.calc']['sig.level']

    algonames = list(set(data_raw['Algorithm']))
    algoruns = {alg: sum(data_raw['Algorithm'] == alg) for alg in algonames}
    algopairs = [f"{row['Alg1']} x {row['Alg2']}" for _, row in data_summary.iterrows()]
    unique_pairs = list(set(algopairs))

    # Initial tests
    my_tests = []
    for pair in unique_pairs:
        rows = data_summary[[f"{row['Alg1']} x {row['Alg2']}" == pair for _, row in data_summary.iterrows()]]
        phi_values = rows['Phi'].values
        d_value = np.mean(phi_values) / np.std(phi_values, ddof=1)

        if test == 't.test':
            result = stats.ttest_1samp(phi_values, popmean=0, alternative=alternative)
            estimate = np.mean(phi_values)
            ci = stats.t.interval(1 - sig_level, len(phi_values)-1, loc=estimate, scale=stats.sem(phi_values))
        elif test == 'wilcoxon':
            result = stats.wilcoxon(phi_values, alternative=alternative)
            estimate = np.median(phi_values)
            ci = [None, None]  # Approximate CI not easily available
        elif test == 'binomial':
            x = phi_values[phi_values != 0]
            n = len(x)
            k = sum(x > 0)
            result = stats.binomtest(k, n, p=0.5, alternative=alternative)
            estimate = result.proportion_estimate if hasattr(result, 'proportion_estimate') else k/n
            ci = result.proportion_ci(confidence_level=1 - sig_level)
        else:
            raise ValueError(f"Unknown test: {test}")

        my_tests.append({
            'comparison': pair,
            'data': rows,
            'test': result,
            'pval': result.pvalue,
            'estimate': estimate,
            'conf.int': ci,
            'd': d_value
        })

    # Sort by p-value
    my_tests.sort(key=lambda x: x['pval'])

    # Re-evaluate with Holm's step-down alpha
    for i, test_obj in enumerate(my_tests):
        phi_values = test_obj['data']['Phi'].values
        alpha = alpha_list

        if test == 't.test':
            result = stats.ttest_1samp(phi_values, popmean=0, alternative=alternative)
            estimate = np.mean(phi_values)
            ci = stats.t.interval(1 - alpha, len(phi_values)-1, loc=estimate, scale=stats.sem(phi_values))
        elif test == 'wilcoxon':
            result = stats.wilcoxon(phi_values, alternative=alternative)
            estimate = np.median(phi_values)
            ci = [None, None]
        elif test == 'binomial':
            x = phi_values[phi_values != 0]
            n = len(x)
            k = sum(x > 0)
            result = stats.binomtest(k, n, p=0.5, alternative=alternative)
            estimate = k / n
            ci = result.proportion_ci(confidence_level=1 - alpha)

        test_obj.update({
            'test': result,
            'pval': result.pvalue,
            'estimate': estimate,
            'conf.int': ci,
            'd': test_obj['d']
        })

    # Print summary
    print("#====================================")
    print(" CAISEr object:")
    print(" Number of instances sampled:", object['N'])
    print(" Number of instances required:", object['N.star'])
    print(" Adequate power:", not object['Underpowered'])
    for alg in algonames:
        print(f" Total runs of {alg}: {algoruns[alg]}")
    print("#====================================")
    print(" Pairwise comparisons of interest:")
    print(" Test:", test)
    print(" H1:", alternative)
    print(" Comparisons:", object['Configuration']['comparisons'])
    print(" Alpha (FWER):", sig_level)
    #print(" Power target:", object['Configuration']['power.target'])
    #print(" Desired power:", object['Configuration']['power'])
    print("#====================================")
    print("Tests using Holm's step-down procedure:")

    stop_flag = False
    for i, test_obj in enumerate(my_tests):
        if not stop_flag and test_obj['pval'] > alpha_list:
            print("\n ----- Stop rejecting H0 at this point -----\n")
            stop_flag = True
        print(f"\n Test {i+1}: {test_obj['comparison']}")
        h0_text = {
            't.test': "mean = 0",
            'wilcoxon': "median = 0",
            'binomial': "prob = 0.5"
        }[test]
        est_text = {
            't.test': "mean",
            'wilcoxon': "median",
            'binomial': "prob"
        }[test]
        print(" H0:", h0_text)
        print(" alpha       =", round(alpha_list, 4))
        print(" p-value     =", round(test_obj['pval'], 4))
        print(f" Est. {est_text} =", round(test_obj['estimate'], 4))
        if test_obj['conf.int'][0] is not None:
            print(" CI{1-alpha} = [", round(test_obj['conf.int'][0], 4), ",", round(test_obj['conf.int'][1], 4), "]")
        print(" d           =", round(test_obj['d'], 4))

    print("#====================================")

    return {
        'test_info': my_tests,
        'algoruns': algoruns,
        'algonames': algonames,
        'algopairs': algopairs
    }

summary=summary_CAISEr(my_results, test='t.test', alternative='two-sided', sig_level=0.05)

# 17 done
def summary_nreps(object):
    print("#====================================")
    print(f"Instance: {object['instance']}")
    print(f"Number of algorithms: {len(object['Nk'])}")

    for algo, runs in object['Nk'].items():
        print(f"{algo}: {runs} runs")

    print(" --------------------")
    print(f"Total runs: {sum(object['Nk'].values())}")
    print(f"Comparisons: {object['comparisons']}")
    print("#====================================\n")

    # Print Diffk matrix or DataFrame rounded to 3 significant digits
    import numpy as np
    import pandas as pd

    if isinstance(object['Diffk'], pd.DataFrame):
        print(object['Diffk'].applymap(lambda x: np.round(x, 3)))
    elif isinstance(object['Diffk'], np.ndarray):
        print(np.round(object['Diffk'], 3))
    else:
        print("Unknown Diffk format")

    print("\n#====================================")

summary_nreps(myreps)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import t

def plot_nreps(
    x,
    instance_name=None,
    latex=False,
    show_SE=True,
    show_CI=True,
    sig_level=0.05,
    show_text=True
):
    # Extract a single instance if plotting from CAISEr object
    if isinstance(x, dict) and x.get("class") == "CAISEr":
        assert isinstance(instance_name, str) and instance_name in x['data.summary']['Instance'].unique()

        df = x['data.summary'][x['data.summary']['Instance'] == instance_name].copy()
        nk = x['data.raw'][x['data.raw']['Instance'] == instance_name]['Algorithm'].value_counts()

        obj = {
            'Diffk': df,
            'Nk': nk.to_dict(),
            'instance': instance_name,
            'class': 'nreps'
        }
        x = obj

    # Assertions
    assert all(k in x for k in ["Diffk", "Nk", "instance"])
    assert 0 < sig_level < 1

    df = x["Diffk"].copy()
    algs = list(x["Nk"].keys())

    # Convert Alg1 and Alg2 if they are indices
    if np.issubdtype(df["Alg1"].dtype, np.integer):
        df["Alg1"] = [algs[i - 1] for i in df["Alg1"]]
    if np.issubdtype(df["Alg2"].dtype, np.integer):
        df["Alg2"] = [algs[i - 1] for i in df["Alg2"]]

    df["CIHW"] = df["SE"] * t.ppf(1 - sig_level / 2, df["N1"] + df["N2"])
    df["pair"] = df["Alg1"] + " x " + df["Alg2"]

    if latex:
        ylabtxt = "$\\phi_{ij}$"
        df["SEtext"] = ["$SE_{ij} = " + str(round(s, 2)) + "$" for s in df["SE"]]
        df["CItext"] = ["$CI_{:.2f} = [{:.2f}, {:.2f}]$".format(
            1 - sig_level, p - c, p + c) for p, c in zip(df["Phi"], df["CIHW"])]
    else:
        ylabtxt = "diff"
        df["SEtext"] = ["SE = " + str(round(s, 2)) for s in df["SE"]]
        df["CItext"] = ["CI({:.2f}) = [{:.2f}, {:.2f}]".format(
            1 - sig_level, p - c, p + c) for p, c in zip(df["Phi"], df["CIHW"])]

    # Start plotting
    plt.figure(figsize=(10, max(4, len(df) * 0.5)))
    sns.set(style="whitegrid")

    # Plot zero line
    plt.axhline(0, color='red', linestyle='--', linewidth=1.2, alpha=0.5)

    if show_CI:
        plt.errorbar(
            x=df["pair"], y=df["Phi"],
            yerr=df["CIHW"], fmt='o', color='black',
            ecolor='red', elinewidth=2, capsize=4, label='CI'
        )

    if show_SE:
        plt.errorbar(
            x=df["pair"], y=df["Phi"],
            yerr=df["SE"], fmt='none', ecolor='blue',
            elinewidth=4, capsize=0, alpha=0.3, label='SE'
        )

    plt.scatter(df["pair"], df["Phi"], s=60, color='black', zorder=3)

    if show_text and show_SE:
        for i, row in df.iterrows():
            plt.text(row["pair"], row["Phi"] + row["SE"] + 0.1,
                     row["SEtext"], ha='center', fontsize=8, color='blue')

    if show_text and show_CI:
        for i, row in df.iterrows():
            plt.text(row["pair"], row["Phi"] - row["CIHW"] - 0.1,
                     row["CItext"], ha='center', fontsize=8, color='red')

    plt.xticks(rotation=90)
    plt.xlabel("Pair")
    plt.ylabel(ylabtxt)
    plt.title(f"Instance: {x['instance']}")
    plt.tight_layout()
    plt.legend()
    plt.show()

plot_nreps(myreps)

def print_CAISEr(obj, echo=True, digits=4, right=True, breakrows=False):
    import pandas as pd
    import numpy as np

    # Basic check to ensure this is a CAISEr-like object
    if 'data.summary' not in obj:
        raise ValueError("Object must contain a 'data.summary' key.")

    df = obj['data.summary'].copy()

    # Break rows into two halves if needed
    if breakrows:
        ninst = len(df)
        breakpoint = int(np.ceil(ninst / 2))
        tophalf = df.iloc[:breakpoint].reset_index(drop=True)
        bottomhalf = df.iloc[breakpoint:].reset_index(drop=True)

        # Pad the bottomhalf if shorter
        if len(tophalf) > len(bottomhalf):
            pad = pd.DataFrame([np.nan] * df.shape[1], index=df.columns).T
            bottomhalf = pd.concat([bottomhalf, pad], ignore_index=True)

        # Combine with visual separator
        my_table = pd.concat([
            tophalf.reset_index(drop=True),
            pd.DataFrame({'|': ['|'] * breakpoint}),
            bottomhalf.reset_index(drop=True)
        ], axis=1)
    else:
        my_table = df

    if echo:
        print("#====================================")
        print(" Summary table of CAISEr object")
        print(my_table.to_string(index=False, float_format=f"{{:.{digits}f}}".format))
        print("#====================================")

    return my_table

print_CAISEr(my_results, breakrows=True)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


def plot_CAISEr(x, figsize=(16, 5), alpha=0.05):
    """
    Python version of plot.CAISEr. Expects the output from run_experiment().
    Parameters:
        x: dict, CAISEr experiment result
        figsize: tuple, size of the figure
        alpha: float, significance level for coloring
    Returns:
        fig: matplotlib figure object
    """
    if not isinstance(x, dict):
        raise ValueError("Input must be a dict returned by run_experiment()")

    summary_df = x["data.summary"].copy()
    raw_df = x["data.raw"].copy()

    # Create a unique comparison column
    summary_df['comparison'] = summary_df['Alg1'] + " x " + summary_df['Alg2']

    # Sort comparisons by mean Phi
    summary_df['mean_phi'] = summary_df['Phi']
    summary_df = summary_df.sort_values('mean_phi', ascending=True)

    # Compute confidence intervals
    ci = 1-alpha
    summary_df['ci_lower'] = summary_df['mean_phi'] - ci
    summary_df['ci_upper'] = summary_df['mean_phi'] + ci

    # Plot 1: Differences with confidence intervals
    fig, axs = plt.subplots(1, 3, figsize=figsize)

    sns.set(style="whitegrid")

    sns.pointplot(
        data=summary_df,
        y="comparison",
        x="mean_phi",
        capsize=0.1,
        color="black",
        err_kws={'linewidth': 1.5},
        ax=axs[0]
    )
    for i, row in summary_df.iterrows():
        axs[0].plot([row['ci_lower'], row['ci_upper']], [i, i], color='black')
        # Color significance: CI doesn't contain 0
        if row['ci_lower'] > 0:
            axs[0].scatter(row['mean_phi'], i, color='green', zorder=3)
        elif row['ci_upper'] < 0:
            axs[0].scatter(row['mean_phi'], i, color='red', zorder=3)
        else:
            axs[0].scatter(row['mean_phi'], i, color='gray', zorder=3)

    axs[0].axvline(0, linestyle='none', color='black', linewidth=0.8)
    axs[0].set_title("Estimated Differences and CIs")
    axs[0].set_xlabel("Phi (difference)")
    axs[0].set_ylabel("")

    # Plot 2: Boxplot of number of runs per algorithm per instance
    counts_df = raw_df.groupby(['Instance', 'Algorithm']).size().reset_index(name='n')
    sns.boxplot(data=counts_df, hue='Algorithm', y='n', ax=axs[1], palette="Set2", legend=False)
    sns.stripplot(data=counts_df, hue='Algorithm', y='n', ax=axs[1], palette='dark:black', size=4, jitter=0.2, legend=False)
    axs[1].set_title("Number of Runs per Instance")
    axs[1].set_ylabel("Runs")
    axs[1].set_xlabel("")

    # Plot 3: Barplot of total runs per algorithm
    total_runs = raw_df['Algorithm'].value_counts().reset_index()
    total_runs.columns = ['Algorithm', 'Count']
    sns.barplot(data=total_runs, hue='Algorithm', y='Count', ax=axs[2], palette="pastel")
    axs[2].set_title("Total Runs per Algorithm")
    axs[2].set_ylabel("Total Runs")
    axs[2].set_xlabel("")

    plt.tight_layout()
    return fig

plot_CAISEr(my_results)